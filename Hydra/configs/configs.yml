# ->->->->-> Primary <-<-<-<-<-
arch: "darts"
exp_name: "temp"
result_dir: "./trained_models"
num_classes: 10
exp_mode: "finetune"
layer_type: "subnet"
init_type: "kaiming_normal"


# ->->->->-> Pruning <-<-<-<-<-
k: 0.01

# ->->->->-> Train <-<-<-<-<-
trainer: "finetune"
epochs: 200 #200 #20 #600
optimizer: "sgd"
lr: 0.01 #0.025 #0.1 
lr_schedule: "cosine"
wd:  0.0004 #0.0003
momentum: 0.9
#warmup
warmup_epochs: 0
warmup_lr: 0.1


# ->->->->-> Eval <-<-<-<-<-
val_method: "base"


# ->->->->-> Dataset <-<-<-<-<-
dataset: CIFAR10
batch_size: 128
test_batch_size: 128
data_dir: "./datasets"
data_fraction: 1.0

# ->->->->-> Semi-supervised training <-<-<-<-<-
semisup_data: "tinyimages"
semisup_fraction: 1.0


# ->->->->-> Adv <-<-<-<-<-
epsilon: 0.031
num_steps: 10
step_size: 0.0078
clip_min: 0
clip_max: 1
distance: "l_inf"
beta: 6.0


# ->->->->-> Misc <-<-<-<-<-
gpu: "0,1"
seed: 12345
print_freq: 100

# ->->->->-> DARTS Architecture search param <-<-<-<-
learning_rate_darts : 0.025
learning_rate_min_darts : 0.001
weight_decay_darts : 0.0003
epochs_darts : 50
init_channels_darts : 16
layers_darts : 8
cutout_darts : False
cutout_length_darts : 16
drop_path_prob_darts : 0.3
grad_clip_darts : 5
train_portion_darts : 0.5
unrolled_darts : True
arch_learning_rate_darts : 0.0003
arch_weight_decay_darts : 0.0001
auxiliary_darts : True
auxiliary_weight_darts : 0.4
arch_darts : "DARTS"
save_darts : 'EXP'
data_darts : '../data'
batch_size_darts : 64
batch_size_test_darts : 64
report_freq : 50
########################################### NEW PARAM  #################################
search_k_darts : 0.01
train_search : False
# ->->->->-> main train DARTS Architecture param <-<-<-<-
trainer_td: "base"
optimizer_td: "sgd"
lr_schedule_td: "cosine"
momentum_td: 0.9
drop_prob_td : 0.2
auxiliary_td : True
auxiliary_weight_td : 0.4
init_channels_td : 36
cutout_td : True
cutout_length_td: 16
layers_td: 20
batch_size_td: 128
batch_size_test_td : 128
epochs_td : 600
learning_rate_td : 0.1
layers_td : 20
arch_td : "DARTS"
#warmup



